{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Policy Gradient Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### The Big Picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('zoOgRoaxGiU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Connections to Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('dJz_p4FKE-g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "To further explore the connections between policy gradient methods and supervised learning, check out Andrej Karpathy's [famous blog post](http://karpathy.github.io/2016/05/31/rl/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Problem Setup\n",
    "\n",
    "Let's rigorously define how policy gradient methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('St9ftvMQ_ks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The equation $U(\\theta)=\\displaystyle\\sum_\\tau\\mathbb{P}(\\tau;\\theta)R(\\tau)$ corresponds to the expected return, note that we've expressed the return $R(\\tau)$ as a function of the trajectory $\\tau$. Then, we calculate the weighted average (where the weights are given by $\\mathbb{P}(\\tau;\\theta)$) of all possible values that the return $R(\\tau)$ can take.\n",
    "\n",
    "#### Why Trajectories !!\n",
    "\n",
    "We're using trajectories instead of episodes because maximizing expected return over trajectories (instead of episodes) lets us search for optimal policies for both episodic and continuing tasks!\n",
    "\n",
    "However, for many episodic tasks, it often makes sense to just use the full episode. In particular, for the case of the video game example described in the videos, reward is only delivered at the end of the episode. In this case, in order to estimate the expected return, the trajectory should correspond to the full episode; otherwise, we don't have enough reward information to meaningfully estimate the expected return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Our goal is to find the values of the weights $\\theta$ in the neural network that maximize the expected return $U(\\theta)=\\displaystyle\\sum_\\tau P(\\tau;\\theta)R(\\tau)$, where $\\tau$ is an arbitrary trajectory. One way to determine the value of $\\theta$ that maximizes this function is through gradient ascent, where the update step appears as follows:\n",
    "\n",
    "$$\\theta\\leftarrow\\theta+\\alpha\\nabla_\\theta$$\n",
    "\n",
    "where $\\alpha$ is the step size that is generally allowed to decay over time. Once we know how to calculate or estimate this gradient, we can repeatedly apply this update step, in the hopes that $\\theta$ converges to the value that maximizes $U(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('o6CI2q3IXEs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Pseudocode\n",
    "The algorithm described in the video is known as **REINFORCE**. The pseudocode is summarized below.\n",
    "\n",
    "1. Use the policy $\\pi_\\theta$ to collect $m$ trajectories $\\left\\{\\tau^{(1)},\\tau^{(2)},\\ldots,\\tau^{(m)}\\right\\}$ with planning horizon $H$. We refer to the $i^{th}$ trajectory as $$\\tau^{(i)} = \\left(s_0^{(i)}, a_0^{(i)}, \\ldots, s_H^{(i)}, a_H^{(i)}, s_{H+1}^{(i)}\\right)$$\n",
    "\n",
    "2. Use the trajectories to estimate the gradient $\\nabla_\\theta U(\\theta)$: $$\\nabla_\\theta U(\\theta) \\approx \\hat{g} := \\frac{1}{m}\\sum_{i=1}^m\\sum_{t=0}^{H}\\nabla_\\theta\\log\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)R\\left(\\tau^{(i)}\\right)$$\n",
    "\n",
    "3. Update the weights of the policy: $\\theta\\leftarrow\\theta+\\alpha\\hat{g}$\n",
    "\n",
    "4. Loop over steps 1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Derivation\n",
    "\n",
    "The derivation of the equation that is used to approximate the gradient is described below.\n",
    "\n",
    "#### Likelihood Ratio Policy Gradient\n",
    "\n",
    "We'll begin by exploring how to calculate the gradient $\\nabla_\\theta U(\\theta)$. The calculation proceeds as follows:\n",
    "\n",
    "\\begin{aligned}\n",
    "  \\nabla_\\theta U(\\theta) &= \\nabla_\\theta \\sum_\\tau P(\\tau;\\theta)R(\\tau) &\\\\\n",
    "                          &= \\sum_\\tau \\nabla_\\theta P(\\tau;\\theta)R(\\tau) &\\\\\n",
    "                          &= \\sum_\\tau \\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)} \\nabla_\\theta P(\\tau;\\theta)R(\\tau) & (1)\\\\\n",
    "                          &= \\sum_\\tau P(\\tau;\\theta) \\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau) &\\\\\n",
    "                          &= \\sum_\\tau P(\\tau;\\theta)\\left[\\nabla_\\theta \\log P(\\tau;\\theta) R(\\tau)\\right] &\\\\\n",
    "                          &= \\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\nabla_\\theta\\log P(\\tau;\\theta)R(\\tau)\\right] &\n",
    "\\end{aligned}\n",
    "\n",
    "The _trick_ in equation (1) is referred to as the **likelihood ratio trick** or **REINFORCE trick**.\n",
    "\n",
    "Likewise, it is common to refer to the gradient as the **likelihood ratio policy gradient**: $\\nabla_\\theta U(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\nabla_\\theta\\log P(\\tau;\\theta)R(\\tau)\\right]$\n",
    "\n",
    "Once weâ€™ve written the gradient as an expected value in this way, it becomes much easier to estimate.\n",
    "\n",
    "#### Sample-Based Estimate\n",
    "\n",
    "We can approximate the likelihood ratio policy gradient with a sample-based average, as follows:\n",
    "\n",
    "$$\\nabla_\\theta U(\\theta)\\approx\\frac{1}{m}\\sum_{i=1}^m\\nabla_\\theta\\log P\\left(\\tau^{(i)};\\theta\\right)R\\left(\\tau^{(i)}\\right)$$\n",
    "where each $\\tau^{(i)}$ is a sampled trajectory.\n",
    "\n",
    "We will further simplify $\\nabla_\\theta\\log P\\left(\\tau^{(i)};\\theta\\right)$ as follows:\n",
    "\\begin{aligned}\n",
    "    \\nabla_\\theta\\log P\\left(\\tau^{(i)};\\theta\\right) &= \\nabla_\\theta\\log\\left[\\prod_{t=0}^{H}P\\left(s_{t+1}^{(i)}|s_{t}^{(i)},a_t^{(i)}\\right)\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)\\right]\\\\\n",
    "                                                              &= \\nabla_\\theta\\left[\\sum_{t=0}^{H}\\log P\\left(s_{t+1}^{(i)}|s_{t}^{(i)},a_t^{(i)}\\right)+\\sum_{t=0}^{H}\\log\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)\\right]\\\\\n",
    "                                                              &= \\nabla_\\theta\\sum_{t=0}^{H}\\log P\\left(s_{t+1}^{(i)}|s_{t}^{(i)},a_t^{(i)}\\right)+\\nabla_\\theta\\sum_{t=0}^{H}\\log\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)\\\\\n",
    "                                                              &= \\nabla_\\theta\\sum_{t=0}^{H}\\log\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)\\\\\n",
    "                                                              &= \\sum_{t=0}^{H}\\nabla_\\theta\\log\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "and obtain our final estimation $$\\nabla_\\theta U(\\theta)\\approx\\frac{1}{m}\\sum_{i=1}^m\\sum_{t=0}^{H}\\nabla_\\theta\\log\\pi_\\theta\\left(a_t^{(i)}|s_t^{(i)}\\right)R\\left(\\tau^{(i)}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### An Example\n",
    "Suppose we are training an agent to play a computer game. There are only two possible action:\n",
    "```\n",
    "   0 = Do nothing\n",
    "   1 = Move\n",
    "```\n",
    "\n",
    "There are three time-steps in each game, and our policy is completely determined by one parameter $\\theta$, such that the action probabilities are defined as follows:\n",
    "\n",
    "$\\begin{align}\n",
    "Pr(\\text{moving})=\\pi_\\theta(1âˆ£\\cdot)&=\\theta\\\\\n",
    "Pr(\\text{doing nothing})=\\pi_\\theta(0âˆ£\\cdot)&=1-\\theta\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "Initially $\\theta=0.5$. Three games are played, the results are:\n",
    "```\n",
    "Game 1: actions: (1,0,1) rewards: (1,0,1)\n",
    "Game 2: actions: (1,0,0) rewards: (0,0,1)\n",
    "Game 3: actions: (0,1,0) rewards: (1,0,1)\n",
    "```\n",
    "Q1. What are the future rewards for the first game?\n",
    " - [ ] (1,0,1)\n",
    " - [ ] (1,0,2)\n",
    " - [ ] (2,0,1)\n",
    " - [x] (2,1,1)\n",
    " - [ ] (1,1,2)\n",
    " \n",
    "Q2. What is the policy gradient computed from the second game, using future rewards?\n",
    " - [x] -2\n",
    " - [ ] -1\n",
    " - [ ] 0\n",
    " - [ ] 1\n",
    " - [ ] 2\n",
    " \n",
    "Q3. Which of these statements are true regarding the 3rd game?\n",
    " - [ ] We can add a baseline -1 point to the rewards, the computed gradient wouldn't change\n",
    " - [x] The contribution to the gradient from the second and third steps cancel each other\n",
    " - [ ] The computed policy gradient from this game is 0\n",
    " - [x] The computed policy gradient from this game is negative\n",
    " - [x] Using the total reward vs future reward give the same policy gradient in this game\n",
    " \n",
    "---\n",
    "\n",
    "Next we will train REINFORCE with OpenAI Gym's Cartpole environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     19
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os.path\n",
    "import numpy as np\n",
    "import random as rand\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "gym.logger.set_level(40)                    # suppress warnings (please remove if gives error)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from utils.ParallelEnvironments import ParallelEnv\n",
    "\n",
    "from IPython.display import display as Display\n",
    "from IPython.display import HTML\n",
    "from pyvirtualdisplay import Display as display\n",
    "display = display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT = 'CartPole-v0'\n",
    "MODEL_FILE = './models/reinforce-cart-pole.pt'\n",
    "RANDOM_SEED = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Train the Agent with REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "\n",
    "env.seed(RANDOM_SEED)\n",
    "rand.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Define the Architecture of the Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size=4, h_size=16, a_size=2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        '''\n",
    "        computes ðœ‹(action|state)\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            states: state vector of shape (1, 4)\n",
    "                    example: [-0.04456399  0.04653909  0.01326909 -0.02099827]\n",
    "        '''\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # tensor([[-0.0445 0.0465 0.0132 -0.0209]])\n",
    "        probs = self.forward(state).to(device)                          # tensor([[0.410 0.589]])\n",
    "        \n",
    "        m = Categorical(probs)          # https://pytorch.org/docs/stable/distributions.html\n",
    "        action = m.sample()             # tensor([1])\n",
    "        log_prob = m.log_prob(action)   # tensor([-0.528])\n",
    "        action = action.item()          # 1\n",
    "\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "policy = Policy().to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### The REINFORCE Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_EVERY = 100\n",
    "TARGET_SCORE = 195.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def reinforce(n_episodes=1000, max_t=1000, gamma=1.0):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "\n",
    "        scores.append(sum(rewards))\n",
    "        scores_deque.append(scores[-1])\n",
    "        \n",
    "        discounts = [gamma**i for i in range(len(rewards)+1)]\n",
    "        R = sum([a*b for a,b in zip(discounts, rewards)])\n",
    "        \n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "\n",
    "        # policy_loss == [tensor([9.5131]), ... , tensor([16.9835])]\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        # policy_loss == tensor(209.5441)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('\\rEpisode {:04d}\\tAverage Score: {:.2f}\\tCurrent Score: {:.2f}'.format(i_episode, np.mean(scores_deque), scores[-1]), end='')\n",
    "        if i_episode % LOG_EVERY == 0:\n",
    "            print('\\r{}'.format(' ' * 120), end='')\n",
    "            print('\\rEpisode {:04d}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) >= TARGET_SCORE:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "            torch.save(policy.state_dict(), MODEL_FILE)\n",
    "            break\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### The Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "scores = reinforce(n_episodes=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Plot the Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Watch a Smart Agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     9
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "policy = Policy().to(device)\n",
    "\n",
    "# load the weights from file\n",
    "map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else 'cpu'\n",
    "policy.load_state_dict(torch.load(MODEL_FILE, map_location=map_location))\n",
    "\n",
    "state = env.reset()\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "for t in range(1000):\n",
    "    action, _ = policy.act(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "REINFORCE can also be used to solve environments with continuous action spaces. For an environment with a continuous action space, the corresponding policy network could have an output layer that parametrizes a continuous probability distribution.\n",
    "\n",
    "For instance, assume the output layer returns the mean $\\mu$ and variance $\\sigma^2$ of a normal distribution. Then in order to select an action, the agent needs only to pass the most recent state $s_t$s as input to the network, and then use the output mean $\\mu$ and variance $\\sigma^2$ to sample from the distribution $a_t\\sim\\mathcal{N}(\\mu,\\sigma^2)$.\n",
    "\n",
    "This should work in theory, but it's unlikely to perform well in practice! To improve performance with continuous action spaces, we'll have to make some small modifications to the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---\n",
    "## PPO\n",
    "\n",
    "### Issues with REINFORCE\n",
    "\n",
    "In practise we often collect **a single trajectory** $\\tau$ to an estimate of the gradient of the expected reward $\\nabla_\\theta U(\\theta)$.\n",
    "\n",
    "There are three main issues with this approach:\n",
    "\n",
    "##### The gradient estimate $\\nabla_\\theta U(\\theta)$ is very noisy. By chance the collected trajectory may not be representative of the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('GCGqT2knFJ0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "##### There is no clear credit assignment. A trajectory may contain many good/bad actions and whether these actions are reinforced depends only on the final total output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('tfZw1moB25Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Why does changing the gradient not change the original goal of maximizing the expected reward?\n",
    "\n",
    "> It turns out that mathematically, ignoring past rewards might change the gradient for each specific trajectory, but it doesn't change the average gradient in an expected sense. So even though the gradient is different during training, on average we are still maximizing the average reward. In fact, the resultant gradient is less noisy, so training using future reward should speed things up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Next, we will go over ways to improve the REINFORCE algorithm and resolve all 3 issues. The solutions will lead us to [PPO](https://arxiv.org/abs/1707.06347), the idea of which was published by the team at OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "###### Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('oHE7u15n7Qg', start=36)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\mathbb{E}_{x\\sim P(x)}{\\large[}f(x){\\large]}&=\\int p(x)f(x)dx\\\\\n",
    "                                             &=\\int\\frac{q(x)}{q(x)}p(x)f(x)dx\\\\\n",
    "                                             &=\\int q(x)\\left(\\frac{p(x)}{q(x)}f(x)\\right)dx\\\\\n",
    "                                             &=\\mathbb{E}_{x\\sim Q(x)}\\left[\\frac{p(x)}{q(x)}f(x)\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "##### The update process is very inefficient! We run the policy once, update once, and then throw away the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('cYPvWriOPIk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('Y-boYZlpO7g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('NRzjGGX6c34')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "YouTubeVideo('qRAUAAWA_kc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The details of PPO was originally published by the team at OpenAI, and their paper is available [here](https://arxiv.org/abs/1707.06347)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "## Learning to play Pong using REINFORCE and PPO\n",
    "\n",
    "Now, we will implement and train a policy to play [atari-pong](https://www.endtoend.ai/envs/gym/atari/#variants), using only the pixels as input. We'll use convolutional neural nets and multiprocessing to implement and train our policy.\n",
    "\n",
    "Watch the [REINFORCE video](https://www.youtube.com/watch?v=eKIjPrQWIgg) and [PPO video](https://www.youtube.com/watch?v=XhfhR7Z01S0) for a detailed code walk-through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT          = 'PongDeterministic-v4'\n",
    "RIGHT                = 4\n",
    "LEFT                 = 5\n",
    "\n",
    "REINFORCE_MODEL_FILE = './models/reinforce-pong.pt'\n",
    "PPO_MODEL_FILE       = './models/ppo-pong.pt'\n",
    "\n",
    "RANDOM_SEED          = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "PongDeterministic does not contain random frameskip, so it's faster to train than the vanilla Pong-v4 environment. More on the different variants of atari enviroments can be found [here](https://www.endtoend.ai/envs/gym/atari/#variants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "\n",
    "env.seed(RANDOM_SEED)\n",
    "rand.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "print('List of available actions: ', env.unwrapped.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "We will only use the actions:\n",
    "```\n",
    "RIGHTFIRE = 4\n",
    "LEFTFIRE  = 5\n",
    "```\n",
    "The `FIRE` part ensures that the game starts again after losing a life. The actions are hard-coded above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "To speed up training, we can simplify the input by cropping the images and use every other pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     9
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def preprocess_single(image, bkg_color = np.array([144, 72, 17])):\n",
    "    '''\n",
    "    preprocess a single frame (210x160x3)\n",
    "    crop image and downsample to 80x80\n",
    "    stack two frames together as input\n",
    "    '''\n",
    "    img = np.mean(image[34:-16:2,::2]-bkg_color, axis=-1) / 255.0\n",
    "    return img\n",
    "\n",
    "def preprocess_batch(images, bkg_color = np.array([144, 72, 17])):\n",
    "    '''\n",
    "    convert outputs of parallelEnv to inputs of pytorch neural net\n",
    "    this is useful for batch processing especially on the GPU\n",
    "    '''\n",
    "    list_of_images = np.asarray(images)\n",
    "    if len(list_of_images.shape) < 5:\n",
    "        list_of_images = np.expand_dims(list_of_images, 1)\n",
    "    # subtract background and crop\n",
    "    list_of_images_prepro = np.mean(list_of_images[:,:,34:-16:2,::2]-bkg_color, axis=-1) / 255.0\n",
    "    batch_input = np.swapaxes(list_of_images_prepro, 0, 1)\n",
    "    # torch.Size([N, len(images), 80, 80]) i.e. (#data_points, #frames, 80, 80)\n",
    "    return torch.from_numpy(batch_input).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### This is what a preprocessed image looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.step(0)\n",
    "for _ in range(20): # get a frame after 20 steps\n",
    "    frame, _, _, _ = env.step(1)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(frame)\n",
    "plt.title('original image')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title('preprocessed image')\n",
    "\n",
    "# 80x80 black and white image\n",
    "plt.imshow(preprocess_single(frame), cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Policy\n",
    " \n",
    "Now, we define the policy. The input is the stack of two different frames (which captures the movement), and the output is a number $\\Pr(right)$, the probability of moving left. Note that $\\quad\\Pr(left)= 1-\\Pr(right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     38,
     39
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    '''\n",
    "    set up a convolutional neural net\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        input: Nx2x80x80 shaped vector i.e. N data/input, 2 frames/channels, each image is 80x80\n",
    "        outout: Nx1 shaped vector representing the probabilities of moving right i.e. P(right)\n",
    "                P(left) = 1 - P(right)\n",
    "        N = 1\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        # in_channel = number of channels in the input image\n",
    "        # out_channel = number of channels in the output feature map = number of filters\n",
    "        # outputsize = (inputsize - kernel_size + stride) / stride  (round up if not an integer)\n",
    "        # 2x80x80 to 4x38x38\n",
    "        self.conv1 = nn.Conv2d(2, 4, kernel_size=6, stride=2, bias=False)\n",
    "        # 4x38x38 to 16x9x9\n",
    "        self.conv2 = nn.Conv2d(4, 16, kernel_size=6, stride=4)\n",
    "        self.size=16*9*9\n",
    "        \n",
    "        # two fully connected layer\n",
    "        self.fc1 = nn.Linear(self.size, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        # Sigmoid \n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # shape of x is 1x2x80x80 i.e. 1 data/input, 2 frames/channels, each image is 80x80\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1,self.size) # flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.sig(self.fc2(x))\n",
    "    \n",
    "\n",
    "class BadPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BadPolicy, self).__init__()\n",
    "        # 2x80x80 to 1x20x20\n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=4, stride=4)\n",
    "        self.size=1*20*20\n",
    "        \n",
    "        # 1 fully connected layer\n",
    "        self.fc = nn.Linear(self.size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        x = F.relu(self.conv(x))\n",
    "        x = x.view(-1,self.size)\n",
    "        return self.sig(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "policy = Policy().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Game visualization\n",
    "\n",
    "Here we define a function that plays a game and shows learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     14
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def animate_frames(frames):\n",
    "    'function to animate a list of frames'\n",
    "    \n",
    "    def display_animation(anim):\n",
    "        plt.close(anim._fig)\n",
    "        return HTML(anim.to_jshtml())\n",
    "\n",
    "    plt.axis('off')\n",
    "    cmap = None if len(frames[0].shape) == 3 else 'Greys' # color option for plotting, use Greys for greyscale\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "    fanim = animation.FuncAnimation(plt.gcf(), lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    Display(display_animation(fanim))\n",
    "\n",
    "\n",
    "def play(env, policy, time=2000, preprocess=None, nrand=5):\n",
    "    '''\n",
    "    play a game and display the animation\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        nrand (int): number of random steps before using the policy\n",
    "        preprocess (func): add the argument preprocess_single to see what the agent sees\n",
    "    '''\n",
    "    env.reset()\n",
    "    env.step(0) # start game\n",
    "    \n",
    "    # perform nrand random steps in the beginning\n",
    "    for _ in range(nrand):\n",
    "        frame1, _, _, _ = env.step(np.random.choice([RIGHT,LEFT]))\n",
    "        frame2, _, _, _ = env.step(0)\n",
    "    \n",
    "    anim_frames = []\n",
    "    \n",
    "    for i in range(time):\n",
    "        frame_input = preprocess_batch([frame1, frame2])\n",
    "        prob = policy(frame_input)\n",
    "            \n",
    "        # RIGHT = 4, LEFT = 5\n",
    "        action = RIGHT if rand.random() < prob else LEFT\n",
    "        frame1, _, _, _ = env.step(action)\n",
    "        frame2, _, is_done, _ = env.step(0)\n",
    "\n",
    "        if preprocess is None:\n",
    "            anim_frames.append(frame1)\n",
    "        else:\n",
    "            anim_frames.append(preprocess(frame1))\n",
    "        \n",
    "        if is_done:\n",
    "            break\n",
    "    animate_frames(anim_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "The `green` agent is supposed to be controlled by the RL algorithm and the `orange` agent is controlled by the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "play(env, policy, time=400) # add the argument \"preprocess=preprocess_single\" to see what the agent sees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Rollout\n",
    "\n",
    "Before we start the training, we need to collect samples. To make things efficient we use parallelized environments to collect multiple trajectories at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def collect_trajectories(envs, policy, tmax=200, nrand=5):\n",
    "    '''\n",
    "    collect trajectories for a parallelized parallelEnv object\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        envs: the parallelEnv object\n",
    "        policy: the policy network\n",
    "        tmax: maximum trajectory length\n",
    "        nrand: number of random steps before using the policy\n",
    "    '''\n",
    "\n",
    "    n = len(envs.ps) # number of parallel processes\n",
    "\n",
    "    #initialize returning lists and start the game!\n",
    "    state_list, reward_list, prob_list, action_list = [], [], [], []\n",
    "    envs.reset()\n",
    "    envs.step([0]*n) # start all parallel environments\n",
    "    \n",
    "    # skip nrand random steps\n",
    "    for _ in range(nrand):\n",
    "        fr1, re1, _, _ = envs.step(np.random.choice([RIGHT, LEFT],n))\n",
    "        fr2, re2, _, _ = envs.step([0]*n)\n",
    "    \n",
    "    for t in range(tmax):\n",
    "        # preprocess_batch properly converts two frames into \n",
    "        # shape (N, 2, 80, 80), the proper input for the policy\n",
    "        # this is required when building CNN with pytorch\n",
    "        batch_input = preprocess_batch([fr1,fr2])\n",
    "        \n",
    "        # pi_old\n",
    "        # probs will only be used for division\n",
    "        # no gradient propagation is required\n",
    "        # so we detach and move it to the cpu\n",
    "        probs = policy(batch_input).squeeze().cpu().detach().numpy()\n",
    "        \n",
    "        action = np.where(np.random.rand(n) < probs, RIGHT, LEFT)\n",
    "        probs = np.where(action==RIGHT, probs, 1.0 - probs)\n",
    "        \n",
    "        # take one action and skip game forward\n",
    "        fr1, re1, is_done, _ = envs.step(action)\n",
    "        fr2, re2, is_done, _ = envs.step([0]*n)\n",
    "\n",
    "        reward = re1 + re2\n",
    "        \n",
    "        # store the result\n",
    "        state_list.append(batch_input)\n",
    "        reward_list.append(reward)\n",
    "        prob_list.append(probs)\n",
    "        action_list.append(action)\n",
    "        \n",
    "        # stop if any of the trajectories is done\n",
    "        # we want all the lists to be retangular\n",
    "        if is_done.any():\n",
    "            break\n",
    "\n",
    "    # return probabilities, states, actions, rewards\n",
    "    return prob_list, state_list, action_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "envs = ParallelEnv(ENVIRONMENT, 4, RANDOM_SEED)\n",
    "prob, state, action, reward = collect_trajectories(envs, policy, tmax=100)\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "assert len(prob) == len(state) and len(state) == len(action) and len(action) == len(reward)\n",
    "\n",
    "print('Length of each trajectory: {}'.format(len(prob)))\n",
    "print('For each each trajectory, at t = 0')\n",
    "print('\\tShape of state space: {}'.format(state[0].shape))\n",
    "print('\\tShape of action space: {}'.format(action[0].shape))\n",
    "print('\\tShape of probabilities: {}'.format(prob[0].shape))\n",
    "print('\\tShape of rewards: {}'.format(reward[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Objective Function for Training\n",
    "Here we define objective functions for training\n",
    "\n",
    "#### REINFORCE\n",
    "We have two **equivalent** choices (A and B).\n",
    "\n",
    "```\n",
    "N (number of parallel agents) = 4\n",
    "T (length of the trajectory)  = 100\n",
    "```\n",
    "\n",
    "  **Choice A**:\n",
    "  1. $\\nabla_\\theta U(\\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}\\frac{1}{T}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)}\\nabla_\\theta\\log\\left(\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)\\right)$\n",
    "  2. $\\nabla_\\theta U(\\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)}\\nabla_\\theta\\log\\left(\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)\\right)$\n",
    "\n",
    "\n",
    "  **Choice B**\n",
    "  1. $\\nabla_\\theta U(\\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}\\frac{1}{T}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)}\\displaystyle\\frac{\\nabla_\\theta\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}$\n",
    "  2. $\\nabla_\\theta U(\\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)}\\displaystyle\\frac{\\nabla_\\theta\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}$\n",
    "\n",
    "\n",
    "  - In **A.1** and **B.1**, usually it's useful to divide by the time since we've normalized our rewards and the time of each trajectory is fixed (`loss_A_1` and `loss_B_1`). The reason behind the extra $\\displaystyle\\frac{1}{T}$ factor in **A.1** and **B.1** is to keep the gradient estimate small lest we may overshhot the local maximum. **B.1** was used by the original author.\n",
    "  - In **B**, make sure that the no_grad is enabled when performing the division\n",
    "    - We recomputed these probabilities $\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)$ as `pi_old` while collecting trajectories and `detach()`-ed them from the gradient computation graph\n",
    "\n",
    "$\\theta_{new}\\leftarrow\\theta_{old}+\\alpha\\cdot\\nabla_\\theta U(\\theta)$\n",
    "\n",
    "---\n",
    "\n",
    "#### PPO\n",
    "Similar to **Choice B** in REINFORCE, we have two non-equivalent choices.\n",
    "\n",
    "1. $U(\\theta',\\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}\\frac{1}{T}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)}\\min\\left\\{\\frac{\\pi_{\\theta'}\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)},{\\rm clip}_{\\epsilon}\\left(\\frac{\\pi_{\\theta'}\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}\\right)\\right\\}$\n",
    "\n",
    "2. $U(\\theta',\\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)}\\min\\left\\{\\frac{\\pi_{\\theta'}\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)},{\\rm clip}_{\\epsilon}\\left(\\frac{\\pi_{\\theta'}\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}\\right)\\right\\}$\n",
    "\n",
    "\n",
    "  - Choice **1** was used by the original author. This is desirable because we have normalized our rewards.\n",
    "  - The ${\\rm clip}_\\epsilon$ function is implemented in pytorch as ```torch.clamp(ratio, 1-epsilon, 1+epsilon)```\n",
    "\n",
    "$\\nabla_{\\theta'} U(\\theta', \\theta)=\\frac{1}{N}{\\displaystyle\\sum^N_{i=1}}{\\displaystyle\\sum^T_{t=1}} {R_t^{future}}^{(i)} \\nabla_{\\theta'}\\min\\left\\{\\frac{\\pi_{\\theta'}\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)},{\\rm clip}_{\\epsilon}\\left(\\frac{\\pi_{\\theta'}\\left(a^{(i)}_t|s^{(i)}_t\\right)}{\\pi_\\theta\\left(a^{(i)}_t|s^{(i)}_t\\right)}\\right)\\right\\}$\n",
    "\n",
    "$\\theta'_{new}\\leftarrow\\theta'_{old}+\\alpha\\cdot\\nabla_{\\theta'} U(\\theta', \\theta)$\n",
    "\n",
    "---\n",
    "\n",
    "The objective functions `surrogate` and `clipped_surrogate` return -ve values because by default the optimizers (*optim.SGD*, *optim.Adam* etc.) use gradient descent $\\theta_{new}\\leftarrow\\theta_{old}-\\alpha\\cdot\\nabla_\\theta U(\\theta)$\n",
    "\n",
    "But _Policy Gradient Methods_ uses gradient ascent $\\theta_{new}\\leftarrow\\theta_{old}+\\alpha\\cdot\\nabla_\\theta U(\\theta)$, so our `surrogate` and `clipped_surrogate` return -ve losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# TODO\n",
    "\n",
    "Evaluate the 6 different equations for the same values of $N$ and $T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     20,
     66
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def states_to_prob(policy, states):\n",
    "    '''\n",
    "    convert states to probability, passing through the policy network\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        states: state vector for the parallel trajectories\n",
    "                shape is (length of trajectory, number of trajectories, stetes encoding ...)\n",
    "                         (length of trajectory, number of trajectories, 2 frames, 80, 80)\n",
    "                         (100, 4, 2, 80, 80)\n",
    "    Returns\n",
    "    ======\n",
    "        vector of probabilities for each state\n",
    "                shape is (length of trajectory, number of trajectories) i.e. (100, 4)\n",
    "    '''\n",
    "    states = torch.stack(states)\n",
    "    policy_input = states.view(-1,*states.shape[-3:]) # policy_input.shape == (400, 2, 80, 80)\n",
    "    return policy(policy_input).view(states.shape[:-3])\n",
    "\n",
    "\n",
    "def surrogate(policy, old_probs, states, actions, rewards, discount = 0.995, beta=0.01):\n",
    "    'returns sum of log-prob, same thing as -policy_loss'\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis]) / std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0 - new_probs)\n",
    "\n",
    "    ratio = new_probs / old_probs  # old_probs == new_probs\n",
    "    \n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan    \n",
    "    entropy = - new_probs * torch.log(old_probs + 1.0e-10) - (1.0 - new_probs) * torch.log(1.0 - old_probs + 1.0e-10)\n",
    "    \n",
    "    regularized_return_A = rewards * torch.log(new_probs) + beta * entropy\n",
    "    regularized_return_B = rewards * ratio + beta * entropy\n",
    "    \n",
    "    expected_reward_A_1 = torch.mean(regularized_return_A)\n",
    "    expected_reward_A_2 = torch.mean(torch.sum(regularized_return_A, dim=0))\n",
    "    expected_reward_B_1 = torch.mean(regularized_return_B)\n",
    "    expected_reward_B_2 = torch.mean(torch.sum(regularized_return_B, dim=0))\n",
    "    \n",
    "    expected_reward = expected_reward_A_2\n",
    "    \n",
    "    # -ve because by default the optimizers (optim.SGD, optim.Adam etc.) use gradient descent\n",
    "    return - expected_reward\n",
    "\n",
    "\n",
    "def clipped_surrogate(policy, old_probs, states, actions, rewards, discount = 0.995, beta=0.01, epsilon=0.1):\n",
    "    'clipped surrogate function, similar as -policy_loss for REINFORCE, but for PPO'\n",
    "    \n",
    "    discount = discount**np.arange(len(rewards))\n",
    "    rewards = np.asarray(rewards)*discount[:,np.newaxis]\n",
    "    \n",
    "    # convert rewards to future rewards\n",
    "    rewards_future = rewards[::-1].cumsum(axis=0)[::-1]\n",
    "    \n",
    "    mean = np.mean(rewards_future, axis=1)\n",
    "    std = np.std(rewards_future, axis=1) + 1.0e-10\n",
    "\n",
    "    rewards_normalized = (rewards_future - mean[:,np.newaxis])/std[:,np.newaxis]\n",
    "    \n",
    "    # convert everything into pytorch tensors and move to gpu if available\n",
    "    actions = torch.tensor(actions, dtype=torch.int8, device=device)\n",
    "    old_probs = torch.tensor(old_probs, dtype=torch.float, device=device)\n",
    "    rewards = torch.tensor(rewards_normalized, dtype=torch.float, device=device)\n",
    "\n",
    "    # convert states to policy (or probability)\n",
    "    new_probs = states_to_prob(policy, states)\n",
    "    new_probs = torch.where(actions == RIGHT, new_probs, 1.0-new_probs)\n",
    "    \n",
    "    # ratio for clipping\n",
    "    ratio = new_probs/old_probs\n",
    "\n",
    "    # clipped function\n",
    "    clip = torch.clamp(ratio, 1 - epsilon, 1 + epsilon)\n",
    "    clipped_surrogate = rewards * torch.min(ratio, clip)\n",
    "\n",
    "    # include a regularization term\n",
    "    # this steers new_policy towards 0.5\n",
    "    # which prevents policy to become exactly 0 or 1\n",
    "    # this helps with exploration\n",
    "    # add in 1.e-10 to avoid log(0) which gives nan  \n",
    "    entropy = -(new_probs * torch.log(old_probs + 1.e-10) + (1.0 - new_probs) * torch.log(1.0 - old_probs + 1.e-10))\n",
    "    \n",
    "    regularized_return = clipped_surrogate + beta * entropy\n",
    "\n",
    "    expected_reward_1 = torch.mean(regularized_return)\n",
    "    expected_reward_2 = torch.mean(torch.sum(regularized_return, dim=0))\n",
    "    \n",
    "    expected_reward = expected_reward_1\n",
    "    \n",
    "    # -ve because by default the optimizers (optim.SGD, optim.Adam etc.) use gradient descent\n",
    "    return -expected_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "policy_loss = surrogate(policy, prob, state, action, reward)\n",
    "clipped_policy_loss = clipped_surrogate(policy, prob, state, action, reward)\n",
    "\n",
    "print(policy_loss, clipped_policy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Training\n",
    "We are now ready to train our policy!\n",
    "\n",
    "Running through all 800 episodes may take up to ~100 minutes even with GPU enabled, otherwise it will take much longer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Hyperparameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR           = 1e-4      # learning rate is 1e-4 or 2e-4 optim.SGD is also possible\n",
    "SGD_EPOCH    = 4         # trajectory re-use count for PPO\n",
    "BETA         = 0.01      # Regularization coefficient\n",
    "\n",
    "LOG_EVERY    = 100       # log scores after this many episodes\n",
    "SAVE_EVERY   = 100       # save the model after this many episodes\n",
    "TARGET_SCORE = 100000.0  # score to solve the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     85
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def train_reinforce(episode=800, tmax=400):\n",
    "    '''\n",
    "    training function for the REINFORCE algorithm\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        episode: number of iterations of the training loop \n",
    "           tmax: maximum length of the collected trajectories\n",
    "    '''\n",
    "    policy = Policy().to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=LR) \n",
    "    \n",
    "    start_episode = 0\n",
    "    beta = BETA\n",
    "    mean_rewards = []\n",
    "    solved = False\n",
    "        \n",
    "    if os.path.isfile(REINFORCE_MODEL_FILE):\n",
    "        # load training checkpoints from file\n",
    "        map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(REINFORCE_MODEL_FILE, map_location=map_location)\n",
    "        policy.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_episode = checkpoint['episode']\n",
    "        beta = checkpoint['beta']\n",
    "        mean_rewards = checkpoint['mean_rewards']\n",
    "        solved = checkpoint['solved']\n",
    "        \n",
    "    if solved:\n",
    "        return mean_rewards\n",
    "        \n",
    "    policy.train()\n",
    "\n",
    "    # initialize environment\n",
    "    envs = ParallelEnv(ENVIRONMENT, 8)\n",
    "    \n",
    "    try:\n",
    "        for e in range(start_episode, start_episode+episode):\n",
    "            # collect trajectories\n",
    "            probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "            total_rewards = np.sum(rewards, axis=0)\n",
    "            loss = surrogate(policy, probs, states, actions, rewards, beta=beta)  # ð‘ˆ(ðœƒ)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()   # âˆ‡ðœƒ ð‘ˆ(ðœƒ)\n",
    "            optimizer.step()  # gradient descent step\n",
    "            del loss\n",
    "\n",
    "            # the regulation term also reduces\n",
    "            # this reduces exploration in later runs\n",
    "            beta *= 0.995\n",
    "\n",
    "            # get the average reward of the parallel environments\n",
    "            mean_rewards.append(np.mean(total_rewards))\n",
    "\n",
    "            # display progress after every iteration        \n",
    "            print('\\r' + ' ' * 120, end='')\n",
    "            print('\\rEpisode: {:04d}/{:04d}\\tAvg. Score: {:.4f} {}\\tScores: {}'.format(e + 1, start_episode + episode, np.mean(total_rewards), ('â†‘' if mean_rewards[-2] <= mean_rewards[-1] else 'â†“') if len(mean_rewards) > 1 else '', total_rewards), end='')\n",
    "            if (e + 1) % LOG_EVERY == 0:\n",
    "                print('\\r' + ' ' * 120, end='')\n",
    "                print('\\rEpisode: {:04d}/{:04d}\\tAvg. Score: {:.4f} {}\\tScores: {}'.format(e + 1, start_episode + episode, np.mean(total_rewards), ('â†‘' if mean_rewards[-2] <= mean_rewards[-1] else 'â†“') if len(mean_rewards) > 1 else '', total_rewards))\n",
    "            if (\n",
    "                (e == start_episode + episode - 1) or\n",
    "                ((e + 1) % SAVE_EVERY == 0) or\n",
    "                np.mean(total_rewards) >= TARGET_SCORE\n",
    "            ):\n",
    "                torch.save({\n",
    "                    'episode': e + 1,\n",
    "                    'model_state_dict': policy.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'beta': beta,\n",
    "                    'mean_rewards': mean_rewards,\n",
    "                    'solved': True if np.mean(total_rewards) >= TARGET_SCORE else False,\n",
    "                }, REINFORCE_MODEL_FILE)\n",
    "                if np.mean(total_rewards) >= TARGET_SCORE:\n",
    "                    print('\\nEnvironment solved in {:04d} episodes!\\tAverage Score: {:.4f}'.format(e + 1, np.mean(total_rewards)))\n",
    "                    break\n",
    "\n",
    "        envs.close()\n",
    "        return mean_rewards\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nTraining Interrupted')\n",
    "        envs.close()\n",
    "        return mean_rewards\n",
    "\n",
    "\n",
    "def train_ppo(episode=800, tmax=400):\n",
    "    '''\n",
    "    training function for the PPO algorithm\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        episode: number of iterations of the training loop \n",
    "           tmax: maximum length of the collected trajectories\n",
    "    '''\n",
    "    \n",
    "    policy = Policy().to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=LR)\n",
    "    \n",
    "    start_episode = 0\n",
    "    beta = BETA\n",
    "    epsilon = 0.1\n",
    "    mean_rewards = []\n",
    "    solved = False\n",
    "    \n",
    "    if os.path.isfile(PPO_MODEL_FILE):\n",
    "        # load training checkpoints from file\n",
    "        map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else 'cpu'\n",
    "        checkpoint = torch.load(PPO_MODEL_FILE, map_location=map_location)\n",
    "        policy.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_episode = checkpoint['episode']\n",
    "        beta = checkpoint['beta']\n",
    "        epsilon = checkpoint['epsilon']\n",
    "        mean_rewards = checkpoint['mean_rewards']\n",
    "        solved = checkpoint['solved']\n",
    "        \n",
    "    if solved:\n",
    "        return mean_rewards\n",
    "    \n",
    "    policy.train()\n",
    "    \n",
    "    # initialize environment\n",
    "    envs = ParallelEnv(ENVIRONMENT, 8)\n",
    "    \n",
    "    try:\n",
    "        for e in range(start_episode, start_episode+episode):\n",
    "            # collect trajectories\n",
    "            probs, states, actions, rewards = collect_trajectories(envs, policy, tmax=tmax)\n",
    "            total_rewards = np.sum(rewards, axis=0)\n",
    "\n",
    "            # gradient ascent step\n",
    "            for _ in range(SGD_EPOCH):\n",
    "                Lsur = clipped_surrogate(policy, probs, states, actions, rewards, beta=beta, epsilon=epsilon) # ð‘ˆ(ðœƒ', ðœƒ)\n",
    "                optimizer.zero_grad()\n",
    "                Lsur.backward()   # âˆ‡ðœƒ' ð‘ˆ(ðœƒ', ðœƒ)\n",
    "                optimizer.step()  # gradient descent step\n",
    "                del Lsur\n",
    "\n",
    "            # the clipping parameter reduces as time goes on\n",
    "            epsilon *= 0.999\n",
    "\n",
    "            # the regulation term also reduces, this reduces exploration in later runs\n",
    "            beta *= 0.995\n",
    "\n",
    "            # get the average reward of the parallel environments\n",
    "            mean_rewards.append(np.mean(total_rewards))\n",
    "\n",
    "            # display progress after every iteration        \n",
    "            print('\\r' + ' ' * 120, end='')\n",
    "            print('\\rEpisode: {:04d}/{:04d}\\tAvg. Score: {:.4f} {}\\tScores: {}'.format(e + 1, start_episode + episode, np.mean(total_rewards), ('â†‘' if mean_rewards[-2] <= mean_rewards[-1] else 'â†“') if len(mean_rewards) > 1 else '', total_rewards), end='')\n",
    "            if (e + 1) % LOG_EVERY == 0:\n",
    "                print('\\r' + ' ' * 120, end='')\n",
    "                print('\\rEpisode: {:04d}/{:04d}\\tAvg. Score: {:.4f} {}\\tScores: {}'.format(e + 1, start_episode + episode, np.mean(total_rewards), ('â†‘' if mean_rewards[-2] <= mean_rewards[-1] else 'â†“') if len(mean_rewards) > 1 else '', total_rewards))\n",
    "            if (\n",
    "                (e == start_episode + episode - 1) or\n",
    "                ((e + 1) % SAVE_EVERY == 0) or\n",
    "                np.mean(total_rewards) >= TARGET_SCORE\n",
    "            ):\n",
    "                torch.save({\n",
    "                    'episode': e + 1,\n",
    "                    'model_state_dict': policy.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'beta': beta,\n",
    "                    'epsilon': epsilon,\n",
    "                    'mean_rewards': mean_rewards,\n",
    "                    'solved': True if np.mean(total_rewards) >= TARGET_SCORE else False,\n",
    "                }, PPO_MODEL_FILE)\n",
    "                if np.mean(total_rewards) >= TARGET_SCORE:\n",
    "                    print('\\nEnvironment solved in {:04d} episodes!\\tAverage Score: {:.2f}'.format(e + 1, np.mean(total_rewards)))\n",
    "                    break\n",
    "\n",
    "        envs.close()\n",
    "        return mean_rewards\n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nTraining Interrupted')\n",
    "        envs.close()\n",
    "        return mean_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "#### start training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_rewards = train_reinforce(episode=600, tmax=320)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Plot average scores across episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_rewards);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Play game after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL_FILE = REINFORCE_MODEL_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT)\n",
    "env.seed(int(time.time()))\n",
    "policy = Policy().to(device)\n",
    "\n",
    "# load the weights from file\n",
    "map_location = (lambda storage, loc: storage.cuda()) if torch.cuda.is_available() else 'cpu'\n",
    "policy.load_state_dict(torch.load(LOAD_MODEL_FILE, map_location=map_location)['model_state_dict'])\n",
    "\n",
    "policy.eval()\n",
    "\n",
    "play(env, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Notes\n",
    "\n",
    "  - The performance for the **REINFORCE** algorithm may be poor. You can try training with a smaller `tmax=100` and more number of `episodes=2000` to see concrete results.\n",
    "  - Try normalizing the future rewards over all the parallel agents, it can speed up training.\n",
    "  - Simpler networks might perform better than more complicated ones! The original input contains `2x80x80=12800` numbers, you might want to ensure that this number steadily decreases at each layer of the neural net.\n",
    "  - It may be beneficial to train multiple epochs, say first using a small `tmax=200` with `500` episodes, and then train again with `tmax=400` with `500` episodes, and then finally with a even larger `tmax`.\n",
    "  - Remember to save your policy after each training phase!\n",
    "  - Try the `Pong-v4` environment, this includes random frameskips and takes longer to train.\n",
    "\n",
    "\n",
    "| REINFORCE Score     |  PPO Score     |\n",
    "|:------------------: | :-------------:|\n",
    "|![](data:image/png;base64,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) | ![](data:image/png;base64,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) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "### Goal of Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "Unlike algorithms such as DQN, which strive to find an excellent policy indirectly through Q-values, policy gradients perform a direct gradient update on a policy to change its parameters, which is what makes it so appealing.\n",
    "\n",
    "Given a trajectory $\\tau$ of length $T$ where $\\tau$ is defined as $\\tau=\\left(s_0,a_0,r_0,s_1,a_1,r_1,\\cdots,s_{Tâˆ’1},a_{Tâˆ’1},r_{Tâˆ’1},s_T\\right)$. The initial state $s_0$ comes from the starting distribution of states, $a_i\\sim\\pi_\\theta(a_i|s_i)$, and $s_i\\sim P(s_i|s_{iâˆ’1},a_{iâˆ’1})$ with $P$ being the transition dynamics of the environment. We define the discounted reward of the trajectory $R(\\tau)=\\displaystyle\\sum_{t=0}^{T-1}\\gamma^tr_t$. The objective of any policy gradient method is then $$\\arg\\max_\\theta \\mathbb{E}_{\\tau\\sim\\pi_\\theta}{\\large[}R(\\tau){\\large]}$$\n",
    "$\\tau\\sim\\pi_\\theta$ under the expectation means the rewards are computed from a trajectory $\\tau$ which was sampled from the policy $\\pi_\\theta$.\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "1. Alternatives include ignoring $\\gamma$ by setting it to $1$, extending $T$ to $\\infty$ if the episodes are infinite-horizon, and so on.\n",
    "2. We don't need to optimize the expected sum of discounted rewards here, rather we try to optimize $\\theta$.\n",
    "3. Vanilla policy gradient methods are so **noisy** because we're **not** maximizing the some objective function ( say $f$) of the parameter $\\theta$ i.e. $\\displaystyle\\arg\\max_\\theta f(\\theta)$. We're rather maximizing a function (say $g$) of a random sample drawn from $f(\\theta)$ i.e. $\\displaystyle\\arg\\max_\\theta g(x)$ where, $x\\sim f(\\theta)$. So, it's sort of *indirect optimization*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "Next: [Actor Critic Methods](./Actor%20Critic%20Methods.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
